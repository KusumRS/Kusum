{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm\n",
      "  Downloading lightgbm-3.1.1-py2.py3-none-win_amd64.whl (754 kB)\n",
      "Requirement already satisfied: scipy in d:\\anaconda\\lib\\site-packages (from lightgbm) (1.4.1)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in d:\\anaconda\\lib\\site-packages (from lightgbm) (0.23.2)\n",
      "Requirement already satisfied: wheel in d:\\anaconda\\lib\\site-packages (from lightgbm) (0.34.2)\n",
      "Requirement already satisfied: numpy in d:\\anaconda\\lib\\site-packages (from lightgbm) (1.19.2)\n",
      "Requirement already satisfied: joblib>=0.11 in d:\\anaconda\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (0.14.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\anaconda\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (2.1.0)\n",
      "Installing collected packages: lightgbm\n",
      "Successfully installed lightgbm-3.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check lightgbm version\n",
    "import lightgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightGBM Ensemble for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate lightgbm algorithm for classification\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from lightgbm import LGBMClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the make_classification() function to create a synthetic binary classification problem with 1,000 examples and 20 input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.55764551, -0.03237545,  0.094302  , ...,  2.09834233,\n",
       "          0.78118829,  0.29187808],\n",
       "        [ 2.7826008 , -0.46094413,  1.89464976, ...,  1.05395455,\n",
       "          0.64310924, -0.17471091],\n",
       "        [ 1.39410728,  1.44814788,  1.29221814, ...,  0.0950894 ,\n",
       "         -1.18508446,  0.76180816],\n",
       "        ...,\n",
       "        [-0.63093694,  0.46552145,  0.29444364, ..., -0.02910069,\n",
       "          0.46439182, -0.20458422],\n",
       "        [-0.2861078 ,  0.21790097,  0.4410491 , ...,  0.16181877,\n",
       "          0.68909249, -2.92675125],\n",
       "        [ 1.16447426,  0.10671813,  1.69623679, ..., -0.46522293,\n",
       "         -1.7552467 ,  0.34599637]]),\n",
       " array([0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,\n",
       "        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,\n",
       "        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_classification()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will evaluate the model using repeated stratified k-fold cross-validation with three repeats and 10 folds. We will report the mean and standard deviation of the accuracy of the model across all repeats and folds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.930 (0.027)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=7)\n",
    "\n",
    "# define the model\n",
    "model = LGBMClassifier()\n",
    "\n",
    "# evaluate the model\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\n",
    "# report performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LightGBM ensemble with default hyperparameters achieves a classification accuracy of about 93% on this test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The LightGBM ensemble is fit on all available data, then the predict() function is used to make predictions on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: 1\n"
     ]
    }
   ],
   "source": [
    "# make predictions using lightgbm for classification\n",
    "\n",
    "# fit the model on the whole dataset\n",
    "model.fit(X, y)\n",
    "\n",
    "# make a single prediction\n",
    "row = [0.2929949,-4.21223056,-1.288332,-2.17849815,-0.64527665,2.58097719,0.28422388,-7.1827928,-1.91211104,2.73729512,0.81395695,3.96973717,-2.66939799,3.34692332,4.19791821,0.99990998,-0.30201875,-4.43170633,-2.82646737,0.44916808]\n",
    "yhat = model.predict([row])\n",
    "print('Predicted Class: %d' % yhat[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightGBM Ensemble for Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the make_regression() function to create a synthetic regression problem with 1,000 examples and 20 input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate lightgbm ensemble for regression\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from lightgbm import LGBMRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-1.27240351,  0.0101501 ,  0.45526745, ...,  0.38990757,\n",
       "          1.35324361, -0.40573697],\n",
       "        [ 0.20816425,  1.91728137, -0.64118619, ..., -0.23562832,\n",
       "          1.0154021 ,  1.37725085],\n",
       "        [-1.86090142,  0.01082649, -1.06225793, ...,  0.10700337,\n",
       "          1.04161986,  0.58396282],\n",
       "        ...,\n",
       "        [ 0.49991999,  0.79966908,  1.31496227, ...,  0.57171019,\n",
       "          0.27228525,  1.30772921],\n",
       "        [ 1.84172684, -0.82230325, -0.32849745, ..., -2.33236965,\n",
       "          0.88887065,  1.1064923 ],\n",
       "        [-0.78794418,  1.1980293 , -0.81050291, ...,  1.68322589,\n",
       "         -0.7963971 , -0.72801595]]),\n",
       " array([  79.64243606,  180.95560533, -184.4263625 , -143.39072724,\n",
       "         123.97389664, -200.74316751, -218.96361356,   91.40022894,\n",
       "        -420.46603403,  252.9528697 ,  425.45620995, -171.4533072 ,\n",
       "          18.95943215, -166.57111778,  139.54578934, -103.06891809,\n",
       "         385.61086256, -292.69095779,  391.78525543, -117.34014409,\n",
       "        -252.78962973, -162.64135745, -350.85321927,   92.84370744,\n",
       "         -91.11210399,  -44.41938182, -204.1530659 ,  184.2400675 ,\n",
       "         124.16061678,  -43.55814903, -195.45736188,   53.46540911,\n",
       "         180.78199209,  -89.56842808,    6.50354895,  221.98086854,\n",
       "          66.46031785,  524.47977473, -167.03296367,  312.03284947,\n",
       "        -310.34695035, -141.47644296, -205.85831735, -475.53027391,\n",
       "        -552.57791271,  -22.73815299, -193.31203059,  238.40542557,\n",
       "          10.208392  , -207.59402886, -210.8893378 ,   30.4177505 ,\n",
       "         208.46748189,   62.78511328,  218.48596416,   30.56475348,\n",
       "        -363.11190866,  188.73889957,  -94.55360717,  180.07387889,\n",
       "         -76.16720022, -122.8363897 ,   94.00820737,  -52.36562893,\n",
       "        -168.58628117,  -38.33610585,  -14.6632587 ,   82.71856783,\n",
       "        -236.37590065, -181.80711805, -415.34139325,  -95.61858184,\n",
       "         -99.57316862,   66.07610738,  -17.02720411,  342.93473635,\n",
       "        -345.25759454,   78.44571396, -254.53375331, -378.33405919,\n",
       "         182.90495665, -302.26111965, -120.05333852, -366.23994047,\n",
       "          94.01529134,   -4.48889551,   -5.70970136,   40.32916936,\n",
       "          31.73633268,   80.58437665,  -27.26029781,   79.53310606,\n",
       "         288.15719895,  -90.66768592,  -18.74389722,  -87.23378635,\n",
       "          30.27674902,   47.43317124,   88.09525095, -106.69479584]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_regression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will evaluate the model using repeated k-fold cross-validation, with three repeats and 10 folds. We will report the mean absolute error (MAE) of the model across all repeats and folds. The scikit-learn library makes the MAE negative so that it is maximized instead of minimized. This means that larger negative MAE are better and a perfect model has a MAE of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: -60.004 (2.887)\n"
     ]
    }
   ],
   "source": [
    "# define dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=20, n_informative=15, noise=0.1, random_state=7)\n",
    "\n",
    "# define the model\n",
    "model = LGBMRegressor()\n",
    "\n",
    "# evaluate the model\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1, error_score='raise')\n",
    "\n",
    "# report performance\n",
    "print('MAE: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the LightGBM ensemble with default hyperparameters achieves a MAE of about 60."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 52\n"
     ]
    }
   ],
   "source": [
    "# fit the model on the whole dataset\n",
    "model.fit(X, y)\n",
    "\n",
    "# make a single prediction\n",
    "row = [0.20543991,-0.97049844,-0.81403429,-0.23842689,-0.60704084,-0.48541492,0.53113006,2.01834338,-0.90745243,-1.85859731,-1.02334791,-0.6877744,0.60984819,-0.70630121,-1.29161497,1.32385441,1.42150747,1.26567231,2.56569098,-0.11154792]\n",
    "yhat = model.predict([row])\n",
    "print('Prediction: %d' % yhat[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LightGBM ensemble is fit on all available data, then the predict() function can be called to make predictions on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
